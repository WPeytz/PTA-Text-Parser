ğŸ“˜ PTA Text Parser

This project extracts, processes, and analyzes legal text from a Preferential Trade Agreement (PTA) â€” specifically the EUâ€“South Korea Free Trade Agreement. The focus is on the Rules of Origin section, which defines the criteria for goods to qualify as originating under the agreement.

ğŸš€ What It Does
	â€¢	Scrapes HTML content from the EUâ€™s legal document portal using requests and BeautifulSoup.
	â€¢	Parses individual paragraphs and assigns them to legal articles.
	â€¢	Cleans and filters to ensure only actual articles (e.g., â€œArticle 3â€) are retained.
	â€¢	Counts words per article to identify verbosity champions.
	â€¢	Searches for keywords related to trade policy using regex (e.g., â€œoriginatingâ€, â€œcumulationâ€).
	â€¢	Visualizes:
	â€¢	Word counts per article
	â€¢	Keyword frequency

ğŸ“‚ Files
	â€¢	PTA_Text_Parser.py â€“ Main script (scraping, parsing, analyzing, plotting).
	â€¢	pta_rules_of_origin.csv â€“ Cleaned dataset of article-paragraph mappings.
	â€¢	wordcount_per_article.png â€“ Bar chart of top 15 longest articles.
	â€¢	keyword_frequencies.png â€“ Bar chart of keyword frequency in text.
    â€¢   pta_explorer.py - 

ğŸ” Example Output

Top 5 Longest Articles:
Article 3    1223 words  
Article 6     999 words  
Article 5     901 words  
Article 1     899 words  
Article 2     814 words  

Keyword Occurrences (with regex):
originating: 17  
cumulation: 1  
tolerance: 0  
HS: 0  

âš™ï¸ How to Run
	1.	Clone the repo:
    git clone https://github.com/yourname/PTA-Text-Parser.git
    cd PTA-Text-Parser
	2.	Install requirements (optional virtualenv highly recommended):
    pip install -r requirements.txt
    3.	Run the parser:
    python PTA_Text_Parser.py

ğŸ§  Future Improvements
	â€¢	Tag paragraphs by type: definition, procedure, exception.
	â€¢	Add section-level parsing beyond â€œArticle Xâ€.
	â€¢	Expand keyword dictionary using NLP techniques.
	â€¢	Integrate Streamlit UI for interactive exploration.

ğŸ’¡ Why This Project?

Because someone had to read the legalese so you didnâ€™t have to â€” and I wanted to prove I can extract meaningful structure and analysis from complex text data using Python and beautiful soup.